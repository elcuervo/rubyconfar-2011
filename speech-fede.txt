Hola! Nosotros vamos a hablar de como ser un data scientist y en
especial como ser un datascientist en ruby.

Mi nombre ... y el es ...


Un poco la agenda para hoy es: 

una introduccion donde vamos a explicar el porque esta copado ser un
data scientist, luego vamos a explicar HDFS y M-R , finalmente vamos a
tomar uo de los frameworks M-R q hay que se llama Hadoop y vamos a ver
como es usar HAdoop en la practica  , vamos a hablar un poco del
ecosistema de Hadoop y bueno luego una cocnlusion.

Basicamente un DS se encarga o mejor dicho tiene que saber como
solucionar TODAS o PARTE de estas. Ahora voy a explicar un poco mas
que siginigican.Yo digo que Data Scientist es una disciplina que encompansa todas esas cosas.

 Hay una charla John Rausser de Amazon que muestra como se puede llegar a ser
Data scientist tanto de la matematica como de la ingenieria. Yo creo
que la mayoria aca tenemos un background de ingenieria, pero de
cualquier manera si no es asi... me gustaria charlar con gente que
viene de las ciencias y que esta interesada en hacer este tipo de
trabjo.(poner grafico)

Quiero explicar estas CUATRO categorias que tenemos ahi : BD trata de
saber que tecnologias y herramientas hay para el procesamiento de big
data y como usarlas
Mining data o Scrapping es otra parte importante generar data de donde
oodamos sacar conclusiones
Data analytics es formular hipotesis y luego tratar de generar modelos
matematicos que prueban estas hipotesis
Finalmente una parte importante y que es sub estimada es que un Data
scientist necesita tener cierta infrastructura para poder realizar su
trabajo de inteligencia y debe estimar y saber cual es el estrcutura
correcta , como disenarla y como crear dicha estructura.

Para poder venir a decirles que tiene que pasarse a trabajos de Data
scientist tenemos este par de qoutes que como ven son la posta xq uno
es el chief economista en google. no es cualquiera es el chief.

Y el segundo lo lei una vez y dsp no pude encontrar el qoute asique lo
reproduje lo mejor que me acordaba

Asique vamos como por el slice 10 y voy a explicar el proposito de
esta charla , que es basicamente xq la profesion del futuro tiene que
ver con entender datos y divertirnos con las ultimas tecnologias

Quiero empezar a hablar de Big data y cuando hablo de big data,
recuerden que era UNO de los temas sobre los cuales queria habar habia
mas..

Bueno cuando hablo de Big data hablo de tecnologias para batch
processing como Hadoop,
tecnologias como Actos y sus Agents para comunicacion; organizacion de procesos
Message queues *bueno los agenets son como message queues
Y quiero terminar hablando de una tecnologia que le estoy dando
bastante bola ultimamente que es para hacer real time systems
tencologias 
refiero a tecnogias como Storm.

Quiero hacer un poco de velocidad (change of gears, creo que ya me
estoy olvidando de como hablar en espanol) y explicar que es Big Data.

Que es big data ?
Bueno big data es un cachphrase una palabra clave para denominar la
parte de computacion de alta performance que se encarga de procesar
sets de datos gigantes que no se podrian procesar eficientemente por
ejemplo en una maquina.

Quien usa big data : muchas de las empresas grandes usan big data para
todo en su core. Facebook por ejemplo lo empezo usando en sus sistemas
de notificaciones a ver a quien le avisaba cuando alguien subia una
foto o subia un comentario. Tambien hacia un algoritmo para ver que
usuarios estan mas cerca y caundo digo mass cerca digo que usuarios
tiene mas afinidad o cosas en comun conmigo y mostrarme el feed de
esos usuarios solamente.

Los datos que usamos los que hacemos Data Scienc son Logs y datos no
normalizados , basicamente buscamos encontrar datos que no estan
dentro del data model del sistema sino que escapan estos. Quizas si
usamos luego los datos que estan en el modelo de datos y lo juntamos
con las cosas que extraemos de los logs. Basicamente el modelo de
datos va estar en una base relacional.

Lo bueno de todo esto es como vamos a ver a continuacion con ciertos
elementos vamos a poder hacer queries sobre hadoop en una syntaxis muy
similar a sequel. 

Lo interesante de estos datos es que son crudos, son puros en cierto
sentido y lo que vamos a hacer es nosotros buscar como es que los
queremos interpretar, parsear, como quieran llamarlo... siempre
buscando ... 

... corelaciones y tambien generalmente conteos. Por ejemplo una
pregunta que nos podemos hacer es cuanta gente me sigue desde hace 3
meses. Eso seria un caso de conteo. Un caso de Correlacion seria,
seria cuanta gente me sigue dsp de q hay un twitt con un hastag
dado. Machine learning seria un cachito mas complicado de explicar
pero me diria basicamente una prediccion de cuantos followeres voy a
tener si sigo twitteando al rate que estoy twitteando.

Bueno podemos inicialmente dividir el problema en dos etapas  , como
guardar big data y como procer esa gran cantidad de datos. Ambas cosas
son importantes y gracias a ellas han surgido nombres que suenan mucho
en el ambiente informatico hoy...

... como por ejemplo guardar , surgieron un monton de solociones para
guardar datos a la que llamamos nosql en forma global. Estas
soluciones en mi pto de vista nacieron x 2 razones basicamente : la
primera es que es a veces el modelo relacional no es el mas obvio para
cierto tipo de data 2) dsp de cierta cantidad de datos los datos no
entran en un servidor y hacer sharding es un dolor de cabeza

Es muy conocido el teorema que llamamos CAP que dice que dado
Consistency, Availability y Partition Tolerance solo podemos elegir dos de
ellas. Por otro lado es imposible tener un sistema distribuido y
ademas que sea Partition Tolerant asique bueno :

Entonces nos la tenemos que jugar: necesitamos Availability o
necesitamos Consistency ? Consistency quiere decir dsp de una
escritura exitosa , los reads futuros van a tener en cuenta esa
escritura. Los sistemas que eligen consistency sobre availability
pueden pasar que en algun momento la base no esta disponible para
aceptar writes y como hacemos en este caso? Probamos dsp , entonces en
este tipo de sistemas decimos que lo mejor que podemos lograr es
eventual consistency. Availability quiere decir basicmente que el
sistema va a estar siepre disponible para writes, pero puede pasar que
si escribimos y dsp leemos un valor el valor que leemos no coincida
con el que recien escribimos. Hay manera de mitigar estos efectos pero
estos efectos siempre van a estar.

Porque usamos ? M-R ?? Basicamente el problema es que hacer un sistema
distribuido es dificil, hay q distribuir los datos, manejar los
fallos, etc... y como una reaccion para MITIGAR esa complejidad unos
ingenieros de google sacaron un paper que proponen usar unos idioms q
vienen del lisp o de lenguajes funcionales que son map y
reduce. Usando Map y Reduce podemos paralelizar en varias maquinas
dividiendo el Dataset como va a explicar elcuervo a continuacion y que
varias maquinas actuen como mappers y varias maquinas (las mismas o
no) reducer.

Por lo tanto lo que necesitamos es un sistema al que le podamos hacer
estas queries y que despues de un tiempo nos responda un resultado ,
bueno lo que necesitamos es un sistema de btch processing qen el que
podamos hacer fire and forget... Hadoop es un sistema que hace eso.

Y tambien hace muchas otras cosas como por ejemplo Map Reduce no es
solamente que yo implemento una clase que extienda la interface MapReduceBase y
luego defino los metodos map y reduce y listo

Entre Map y reduce  pasan muchas cosas como por ejemplo de cada mapper
se hace un fetch a los datos que este produjo, luego se copian los
datos a las otras maquinas para que a la entrada de los reducers
tengan todos la data completa producida por los mappers y luego se
junta esta data y finalmente se hace el reduce

Hadoop tiene un scheduler y herramientas de Serializacion
----


Como les comente anteriormente se puede hacer queries a la base con
ciertos lenguajes que se llaman Pig, Hive o mi preferido Cascalog, que
corre en Clojure

En Ruby lo que he visto es que mucha gente usa templates ruby para
modificar queries en Hive o Pig... lo cual a mi mucho no me
atrae. Pero seria una manera de hacerlo.


Cascalog se le ocurrio a un ingeniero de Twitter que vio que los
lenguajes declarativos eran muy bueno para la programacion distribuida
y decidio implementar sobre hadoop una interfaz declarativa ayudado
por Hadoop para hacer programacion distribuida.

Lo bueno es que en el medio de codigo Cascalog puedo poner Codigo
Cojure, porque realmente Cascalog es un embedded DSL no un external
DSL entonces listo puedo mezclar Cascalog con mi lenguaje de
programacion... puedo hacer cosas que inicialmente Cascalog no estaba
disenado para hacer...
