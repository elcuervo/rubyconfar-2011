Basicamente un DS se encarga o mejor dicho tiene que saber como
solucionar TODAS o PARTE de estas. 
Yo digo que es una disciplina que encompansa todas esas cosas. Hay una
charla John Rausser de Amazon que muestra como se puede llegar a ser
Data scientist tanto de la matematica como de la ingenieria. Yo creo
que la mayoria aca tenemos un background de ingenieria, pero de
cualquier manera si no es asi... me gustaria charlar con gente que
viene de las ciencias y que esta interesada en hacer este tipo de
trabjo.

Quiero explicar estas CUATRO categorias que tenemos ahi : BD trata de
saber que tecnologias y herramientas hay para el procesamiento de big
data y como usarlas
Mining data o Scrapping es otra parte importante generar data de donde
oodamos sacar conclusiones
Data analytics es formular hipotesis y luego tratar de generar modelos
matematicos que prueban estas hipotesis
Finalmente una parte importante y que es sub estimada es que un Data
scientist necesita tener cierta infrastructura para poder realizar su
trabajo de inteligencia y debe estimar y saber cual es el estrcutura
correcta , como disenarla y como crear dicha estructura.

Para poder venir a decirles que tiene que pasarse a trabajos de Data
scientist tenemos este par de qoutes que como ven son la posta xq uno
es el chief economista en google. no es cualquiera es el chief.

Y el segundo lo lei una vez y dsp no pude encontrar el qoute asique lo
reproduje lo mejor que me acordaba

Asique vamos como por el slice 10 y voy a explicar el proposito de
esta charla , que es basicamente xq la profesion del futuro tiene que
ver con entender datos y divertirnos con las ultimas tecnologias

Quiero empezar a hablar de Big data y cuando hablo de big data,
recuerden que era UNO de los temas sobre los cuales queria habar habia
mas..

Bueno cuando hablo de Big data hablo de tecnologias para batch
processing como Hadoop,
tecnologias como Actos y sus Agents para comunicacion; organizacion de procesos
Message queues *bueno los agenets son como message queues
Y quiero terminar hablando de una tecnologia que le estoy dando
bastante bola ultimamente que es para hacer real time systems
tencologias 
refiero a tecnogias como Storm.

Quiero empezar hablando de la vedette de todo esto que es Hadoop.

Hadoop y sus experiencias con Hadoop las va a contar mas adelante
elCuervo pero bueno queria comentarles algo de teoria basica para
ver donde estamos parados
Hadoop ME GUSTA definirlo como su engine y su filesystem HDFS. Si bien
el Map reduce corre sobre el filesystem HDFS en la mayoria de los
casos , y casi siempre toman como unidad , a mi me gusta separarlos.

Quiero hacer un poco de velocidad (change of gears, creo que ya me
estoy olvidando de como hablar en espanol)y explicar que es Big Data.

Que es big data ?
Bueno big data es un cachphrase una palabra clave para denominar la
parte de computacion de alta performance que se encarga de procesar
sets de datos gigantes que no se podrian procesar eficientemente por
ejemplo en una maquina.

Quien usa big data : muchas de las empresas grandes usan big data para
todo en su core. Facebook por ejemplo lo empezo usando en sus sistemas
de notificaciones a ver a quien le avisaba cuando alguien subia una
foto o subia un comentario. Tambien hacia un algoritmo para ver que
usuarios estan mas cerca y caundo digo mass cerca digo que usuarios
tiene mas afinidad o cosas en comun conmigo y mostrarme el feed de
esos usuarios solamente.

Los datos que usamos los que hacemos Data Scienc son Logs y datos no
normalizados , basicamente buscamos encontrar datos que no estan
dentro del data model del sistema sino que escapan estos. Quizas si
usamos luego los datos que estan en el modelo de datos y lo juntamos
con las cosas que extraemos de los logs. Basicamente el modelo de
datos va estar en una base relacional.

Lo bueno de todo esto es como vamos a ver a continuacion con ciertos
elementos vamos a poder hacer queries sobre hadoop en una syntaxis muy
similar a sequel. 

Lo interesante de estos datos es que son crudos, son puros en cierto
sentido y lo que vamos a hacer es nosotros buscar como es que los
queremos interpretar, parsear, como quieran llamarlo... siempre
buscando ... 

... corelaciones y tambien generalmente conteos. Por ejemplo una
pregunta que nos podemos hacer si vemos el mercado inmobiliario es
preguntarnos que tanto influye el tamano de la casa en si se vende o
no. O el precio de la casa. O mejor hacemos una funcion que tome el
tamano de la casa y el precio de la casa y nos diga si la casa se va a
vender o no.

A veces estas relaciones no nos sirven y solo nos sirve saber cuantos
usuarios re-twittearon cierto tweet (ponele que estamos subscriptos al
twitter firehose)t este es como un sub-caso del
anterior y obviamente es mas facil.

Por lo tanto lo que necesitamos es un sistema al que le podamos hacer
estas queries y que despues de un tiempo nos responda un resultado ,
bueno lo que necesitamos es un sistema de btch processing qen el que
podamos hacer fire and forget... Hadoop es un sistema que hace eso.

Y tambien hace muchas otras cosas como por ejemplo Map Reduce no es
solamente que yo implemento una clase que extienda la interface MapReduceBase y
luego defino los metodos map y reduce y listo

Sino que Hadoop nos provee un paso en el medio que es super importante
que lo va a explciar elcuervo a continuacion que se llama shuffle

Dentro de shuffle pasan muchas cosas como por ejemplo de cada mapper
se hace un fetch a los datos que este produjo, luego se copian los
datos a las otras maquinas para que a la entrada de los reducers
tengan todos la data completa producida por los mappers y luego se
junta esta data y finalmente se hace el reduce

Hadoop tiene un scheduler y herramientas de Serializacion
----


Como les comente anteriormente se puede hacer queries a la base con
ciertos lenguajes que se llaman Pig, Hive o mi preferido Cascalog, que
corre en Clojure

En Ruby lo que he visto es que mucha gente usa templates ruby para
modificar queries en Hive o Pig... lo cual a mi mucho no me
atrae. Pero seria una manera de hacerlo.

Clojure es un lenguaje de programacion funcional que corre en la
JVM. Es un LISP-1 con bueno soporte higienico de macros lo que hace
que podamos hacer metaprogramacion de manera facil y limpia.


Cascalog se le ocurrio a un ingeniero de Twitter que vio que los
lenguajes declarativos eran muy bueno para la programacion distribuida
y decidio implementar sobre hadoop una interfaz declarativa ayudado
por Hadoop para hacer programacion distribuida.

Lo bueno es que en el medio de codigo Cascalog puedo poner Codigo
Cojure, porque realmente Cascalog es un embedded DSL no un external
DSL entonces listo puedo mezclar Cascalog con mi lenguaje de
programacion... puedo hacer cosas que inicialmente Cascalog no estaba
disenado para hacer...


... Como por ejemplo algoritmos de Machine learning... 

Ahora lo que si estaba disenado para hacer cascalog es hacer queries a
set de datos gigantes por lo que eso ya viene out of the box

Una query de Cascalog se ve asi :

q es basicamente una query que dice sacame por stdout primer argumento
segundo argumento las personas ... el tercer argumento es lo que se
llama un generador que me dice que las tuplas que componen los datos
son de la forma primero la persona y dsp la edad ... y la query
entonces me dice decime las personas en tu data que tienen menos de 30

El ejemplo de cascalog para gradient descent lo voy a mostrar dsp de
explicar un poco de que se trata machine learning

----

El scope de la charla no es ensenar machine learning sino que es
mostrar como se usa machine learning en el ambito practico de data
science y luego mencionar un par de tecnicas comunes


Machine learning es una rama de Inteligencia Artiicial que se
concentra en datos empiricos , o sea datos anteriores para determinar
comportamientos.

Hay basicamente dos subramas dentro de Machine learning lo que
llamamos supervised learning y unspervidsed.
Supervised learnig requiere la precencia de algun experto, por ejemplo
un humano o un grupo de humanos, que pueda clasificar una pequena parte de nuestra data de
ante mano y decir por ejemplo este es de una clase y este es de otra.

Regresion es basicamente cuando queremos predecir un valor numerico
con respecto a un trainingdata

Y lo que vamos a obtener es algo asiP
Vamos a obtener un gran nro de puntos
Y sobre ellos vamos a obtener una funcion lineal (cuando hablamos de
linear regression) y sobre esa funcion lineal podemos deducir mediante
regresion si hay un punto nuevo, cual va  a ser e valor de ese punto.


Clasificacion es un un poco diferente aca vamos a devuelta tener un
set de training y lo que vamos a buscar es un hiperplano clasificador
en este caso una recta que me diga segun los datos si tengo datos
nuevos si va a ser de una clase o no.

En el caso que se plantea en el curso de Standford y si la gente que lo
esta viendo se acordara que X1 es por ejemplo el tamno y X2 es por
ejemplo el precio. Lo que queremos clasificar o inferir es dado un
tamano y un precio si se vendio o no.

Denuevo repetido para la gente que esta haciendo el curso de Standford
pero hay dos tipos de algoritmos Online y Offline. Offline quiere
decir que cada vez que agregamos un punto de training o sea que le
ponemos una etiqueta a un punto dado , tenemos que recalcular toda
nuesta regresion o clasificacion. Online quiere decir que podemos
agregar un punto y nuestra inferencia va a cambiar interactivamente e
iterativmanete.

EL ejemplo que quiero dar de clasificacion es gradient descent que es 
basicamente n algoritmo que va paso a paso mejorando la prediccion
tomando como input los pesos de los puntos que lo rodean y moviendose
en el camino de la menor resistencia. Este camino esta dado por la
direccion de menor resistencia o sea el camino donde podemos minimizar
la funcion lineal, la recta, esta funcion derivada en cada una de las
variables o sea el gradiente. En la grafica podemos ver el primer paso 
de una iteracion gradient desecent como podemos ver ya se va acercando
a la recta ideal pero todavia no. El final seria como la slide
anterior o sea esta (anterior)

DEMO
