Hola! Nosotros vamos a hablar de como ser un data scientist y en
especial como ser un datascientist en ruby.

Mi nombre ... y el es ...


Un poco la agenda para hoy es: 

una introduccion donde vamos a explicar el porque esta copado ser un
data scientist, luego vamos a explicar HDFS y M-R , finalmente vamos a
tomar uo de los frameworks M-R q hay que se llama Hadoop y vamos a ver
como es usar HAdoop en la practica  , vamos a hablar un poco del
ecosistema de Hadoop y bueno luego una cocnlusion.

Basicamente un DS se encarga o mejor dicho tiene que saber como
solucionar TODAS o PARTE de estas. Ahora voy a explicar un poco mas
que siginigican.Yo digo que Data Scientist es una disciplina que encompansa todas esas cosas.

 Hay una charla John Rausser de Amazon que muestra como se puede llegar a ser
Data scientist tanto de la matematica como de la ingenieria. Yo creo
que la mayoria aca tenemos un background de ingenieria, pero de
cualquier manera si no es asi... me gustaria charlar con gente que
viene de las ciencias y que esta interesada en hacer este tipo de
trabjo.(poner grafico)

Quiero explicar estas CUATRO categorias que tenemos ahi : BD trata de
saber que tecnologias y herramientas hay para el procesamiento de big
data y como usarlas
Mining data o Scrapping es otra parte importante generar data de donde
oodamos sacar conclusiones
Data analytics es formular hipotesis y luego tratar de generar modelos
matematicos que prueban estas hipotesis
Finalmente una parte importante y que es sub estimada es que un Data
scientist necesita tener cierta infrastructura para poder realizar su
trabajo de inteligencia y debe estimar y saber cual es el estrcutura
correcta , como disenarla y como crear dicha estructura.

Para poder venir a decirles que tiene que pasarse a trabajos de Data
scientist tenemos este par de qoutes que como ven son la posta xq uno
es el chief economista en google. no es cualquiera es el chief.

Y el segundo lo lei una vez y dsp no pude encontrar el qoute asique lo
reproduje lo mejor que me acordaba

Asique vamos como por el slice 10 y voy a explicar el proposito de
esta charla , que es basicamente xq la profesion del futuro tiene que
ver con entender datos y divertirnos con las ultimas tecnologias

Quiero empezar a hablar de Big data y cuando hablo de big data,
recuerden que era UNO de los temas sobre los cuales queria habar habia
mas..

Bueno cuando hablo de Big data hablo de tecnologias para batch
processing como Hadoop,
tecnologias como Actos y sus Agents para comunicacion; organizacion de procesos
Message queues *bueno los agenets son como message queues
Y quiero terminar hablando de una tecnologia que le estoy dando
bastante bola ultimamente que es para hacer real time systems
tencologias 
refiero a tecnogias como Storm.

Quiero hacer un poco de velocidad (change of gears, creo que ya me
estoy olvidando de como hablar en espanol) y explicar que es Big Data.

Que es big data ?
Bueno big data es un cachphrase una palabra clave para denominar la
parte de computacion de alta performance que se encarga de procesar
sets de datos gigantes que no se podrian procesar eficientemente por
ejemplo en una maquina.

Quien usa big data : muchas de las empresas grandes usan big data para
todo en su core. Facebook por ejemplo lo empezo usando en sus sistemas
de notificaciones a ver a quien le avisaba cuando alguien subia una
foto o subia un comentario. Tambien hacia un algoritmo para ver que
usuarios estan mas cerca y caundo digo mass cerca digo que usuarios
tiene mas afinidad o cosas en comun conmigo y mostrarme el feed de
esos usuarios solamente.

Los datos que usamos los que hacemos Data Scienc son Logs y datos no
normalizados , basicamente buscamos encontrar datos que no estan
dentro del data model del sistema sino que escapan estos. Quizas si
usamos luego los datos que estan en el modelo de datos y lo juntamos
con las cosas que extraemos de los logs. Basicamente el modelo de
datos va estar en una base relacional.

Lo bueno de todo esto es como vamos a ver a continuacion con ciertos
elementos vamos a poder hacer queries sobre hadoop en una syntaxis muy
similar a sequel. 

Lo interesante de estos datos es que son crudos, son puros en cierto
sentido y lo que vamos a hacer es nosotros buscar como es que los
queremos interpretar, parsear, como quieran llamarlo... siempre
buscando ... 

... corelaciones y tambien generalmente conteos. Por ejemplo una
pregunta que nos podemos hacer es cuanta gente me sigue desde hace 3
meses. Eso seria un caso de conteo. Un caso de Correlacion seria,
seria cuanta gente me sigue dsp de q hay un twitt con un hastag
dado. Machine learning seria un cachito mas complicado de explicar
pero me diria basicamente una prediccion de cuantos followeres voy a
tener si sigo twitteando al rate que estoy twitteando.

Bueno podemos inicialmente dividir el problema en dos etapas  , como
guardar big data y como procer esa gran cantidad de datos. Ambas cosas
son importantes y gracias a ellas han surgido nombres que suenan mucho
en el ambiente informatico hoy...

... como por ejemplo guardar , surgieron un monton de solociones para
guardar datos a la que llamamos nosql en forma global. Estas
soluciones en mi pto de vista nacieron x 2 razones basicamente : la
primera es que es a veces el modelo relacional no es el mas obvio para
cierto tipo de data 2) dsp de cierta cantidad de datos los datos no
entran en un servidor y hacer sharding es un dolor de cabeza

Es muy conocido el teorema que llamamos CAP que dice que dado
Consistency, Availability y Partition Tolerance solo podemos elegir dos de
ellas. Por otro lado es imposible tener un sistema distribuido y
ademas que sea Partition Tolerant asique bueno :

Entonces nos la tenemos que jugar: necesitamos Availability o
necesitamos Consistency ? Consistency quiere decir dsp de una
escritura exitosa , los reads futuros van a tener en cuenta esa
escritura. Los sistemas que eligen consistency sobre availability
pueden pasar que en algun momento la base no esta disponible para
aceptar writes y como hacemos en este caso? Probamos dsp , entonces en
este tipo de sistemas decimos que lo mejor que podemos lograr es
eventual consistency. Availability quiere decir basicmente que el
sistema va a estar siepre disponible para writes, pero puede pasar que
si escribimos y dsp leemos un valor el valor que leemos no coincida
con el que recien escribimos. Hay manera de mitigar estos efectos pero
estos efectos siempre van a estar.

Porque usamos ? M-R ?? Basicamente el problema es que hacer un sistema
distribuido es dificil, hay q distribuir los datos, manejar los
fallos, etc... y como una reaccion para MITIGAR esa complejidad unos
ingenieros de google sacaron un paper que proponen usar unos idioms q
vienen del lisp o de lenguajes funcionales que son map y
reduce. Usando Map y Reduce podemos paralelizar en varias maquinas
dividiendo el Dataset como va a explicar elcuervo a continuacion y que
varias maquinas actuen como mappers y varias maquinas (las mismas o
no) reducer.

Hadoop es el sistema mas usado acutalmente para usar este paradigma de
M-R.
Hadoop consiste basicamente de dos partes, primero del motor o engine
Map-Reduce que a su vez corre por encima del filesystem HDFS. Esto lo
va a explicar mas en detalle elcuervo pero otra cosa que quiero
mencionar antes de dejarlos con el es que Hadoop nos da mucho mas que
una implementacion que podemos overridear de map y reduce nos da todo
el trabajo que va por debajo de sincronizar las maquinas  , copiar los
datos entre ellas etc.
----
Queria hablar un poco de M-R en la practica, si vamos a realmente a
trabajar sobre este paradigma . Lo primero que vamos a descubrir es
que vamos a seguramente necesitar mas de un M-R encadenado. Por lo
tanto una de las mejores cosas es usar un sistemas de queries que te
arme los flujos M-R automaticamente. Hay varios Pig , Hive, que son
creados por Yahoo, luego hay uno que anda bien en Jruby llamado
Cascading y luego esta Cascalog que es mi favorito que usa Clojure que
es mi lenguaje favorito junto a Ruby.

Otra cosa interesante es que recumiendo no usar hadoop streaming, no
solamente hay un penalty de perfermonace sino que es mucho mas
incomodo de trabajar que si trabajas en un lenguaje de la jvm que
pueda correr automaticamente queries desde tu ide o editor.

Hay varios proyectos dentro del ecosistema de Hadoo que son muy
buenos, y recomeindo saber que son y para que sirven para no re
inventar la rueda, seguramente los necesitemos si vamos a hacer un
poryecto grande.

Algunos ejemplos son Zookeeper que es para orquestar un cluster. Flume
que es para subir datos no-normalizados (logs) a un cluster. Sqoop que
es para pasar de RDBMS a un HDFS y Oozie un sistema para crear
workflows genericos parecido a Cascading.

Dsp hay dos bases que trabajan muy bien con HDFS que implementan big
table , por ejemplo HBase que es 100$ consistente y Cassandra que es
100%a vailable.

Otra cosa importante es realmente ahcer proyectos interesantes,
realmente recomendar Hadoop como una alternativa interesante si vas a
hacer consultas de negocios por ejemplo a datos no normalizados. Otro
uso interesante es Machine Learning, una buena introduccion a ML es el
curso de standford que varios de aca se que eetamos ahciendo.

---

Conclusion

No se asusten si fuimos de 0 a 100 en 25 minuts, no tienen porqu
entender todo pero si es interesante que se vayan con un pantallaso de
lo que significa trabajar en big data. Y mas importante ...
... que ser un data scientist es cool
Hay muchos lenguajes para ser un data scientist y ruby es un buen
lenguaje para empezar, puede ser que si avanzas un poco no sea ideal
pero para mquetar un poco esta bueno. Y tambien es importante saber
que Hadoop y otras herramientas nos hacen posible ir contra problemas
quea ntes pensabamos que no se podian atacar facilmente.
