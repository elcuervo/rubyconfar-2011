Basicamente un DS se encarga o mejor dicho tiene que saber como
solucionar TODAS o PARTE de estas. 
Yo digo que es una disciplina que encompansa todas esas cosas. Hay una
charla John Rausser de Amazon que muestra como se puede llegar a ser
Data scientist tanto de la matematica como de la ingenieria. Yo creo
que la mayoria aca tenemos un background de ingenieria, pero de
cualquier manera si no es asi... me gustaria charlar con gente que
viene de las ciencias y que esta interesada en hacer este tipo de
trabjo.

Quiero explicar estas CUATRO categorias que tenemos ahi : BD trata de
saber que tecnologias y herramientas hay para el procesamiento de big
data y como usarlas
Mining data o Scrapping es otra parte importante generar data de donde
oodamos sacar conclusiones
Data analytics es formular hipotesis y luego tratar de generar modelos
matematicos que prueban estas hipotesis
Finalmente una parte importante y que es sub estimada es que un Data
scientist necesita tener cierta infrastructura para poder realizar su
trabajo de inteligencia y debe estimar y saber cual es el estrcutura
correcta , como disenarla y como crear dicha estructura.

Para poder venir a decirles que tiene que pasarse a trabajos de Data
scientist tenemos este par de qoutes que como ven son la posta xq uno
es el chief economista en google. no es cualquiera es el chief.

Y el segundo lo lei una vez y dsp no pude encontrar el qoute asique lo
reproduje lo mejor que me acordaba

Asique vamos como por el slice 10 y voy a explicar el proposito de
esta charla , que es basicamente xq la profesion del futuro tiene que
ver con entender datos y divertirnos con las ultimas tecnologias

Quiero empezar a hablar de Big data y cuando hablo de big data,
recuerden que era UNO de los temas sobre los cuales queria habar habia
mas..

Bueno cuando hablo de Big data hablo de tecnologias para batch
processing como Hadoop,
tecnologias como Actos y sus Agents para comunicacion; organizacion de procesos
Message queues *bueno los agenets son como message queues
Y quiero terminar hablando de una tecnologia que le estoy dando
bastante bola ultimamente que es para hacer real time systems
tencologias 
refiero a tecnogias como Storm.

Quiero empezar hablando de la vedette de todo esto que es Hadoop.

Hadoop y sus experiencias con Hadoop las va a contar mas adelante
elCuervo pero bueno queria comentarles algo de teoria basica para
ver donde estamos parados
Hadoop ME GUSTA definirlo como su engine y su filesystem HDFS. Si bien
el Map reduce corre sobre el filesystem HDFS en la mayoria de los
casos , y casi siempre toman como unidad , a mi me gusta separarlos.

Quiero hacer un poco de velocidad (change of gears, creo que ya me
estoy olvidando de como hablar en espanol)y explicar que es Big Data.

Que es big data ?
Bueno big data es un cachphrase una palabra clave para denominar la
parte de computacion de alta performance que se encarga de procesar
sets de datos gigantes que no se podrian procesar eficientemente por
ejemplo en una maquina.

Quien usa big data : muchas de las empresas grandes usan big data para
todo en su core. Facebook por ejemplo lo empezo usando en sus sistemas
de notificaciones a ver a quien le avisaba cuando alguien subia una
foto o subia un comentario. Tambien hacia un algoritmo para ver que
usuarios estan mas cerca y caundo digo mass cerca digo que usuarios
tiene mas afinidad o cosas en comun conmigo y mostrarme el feed de
esos usuarios solamente.

Un 

El scope de la charla no es ensenar machine learning sino que es
mostrar como se usa machine learning en el ambito practico de data
science y luego mencionar un par de tecnicas comunes

Machine learning es una rama de Inteligencia Artiicial que se
concentra en datos empiricos , o sea datos anteriores para determinar
comportamientos.

Hay basicamente dos subramas dentro de Machine learning lo que
llamamos supervised learning y unspervidsed.
Supervised learnig requiere la precencia de algun experto, por ejemplo
un humano o un grupo de humanos, que pueda clasificar una pequena parte de nuestra data de
ante mano y decir por ejemplo este es de una clase y este es de otra.

Regresion es basicamente cuando queremos predecir un valor numerico
con respecto a un trainingdata

Y lo que vamos a obtener es algo asiP
Vamos a obtener un gran nro de puntos
Y sobre ellos vamos a obtener una funcion lineal (cuando hablamos de
linear regression) y sobre esa funcion lineal podemos deducir mediante
regresion si hay un punto nuevo, cual va  a ser e valor de ese punto.


Clasificacion es un un poco diferente aca vamos a devuelta tener un
set de training y lo que vamos a buscar es un hiperplano clasificador
en este caso una recta que me diga segun los datos si tengo datos
nuevos si va a ser de una clase o no.

En el caso que se plantea en el curso de Standford y si la gente que lo
esta viendo se acordara que X1 es por ejemplo el tamno y X2 es por
ejemplo el precio. Lo que queremos clasificar o inferir es dado un
tamano y un precio si se vendio o no.

Denuevo repetido para la gente que esta haciendo el curso de Standford
pero hay dos tipos de algoritmos Online y Offline. Offline quiere
decir que cada vez que agregamos un punto de training o sea que le
ponemos una etiqueta a un punto dado , tenemos que recalcular toda
nuesta regresion o clasificacion. Online quiere decir que podemos
agregar un punto y nuestra inferencia va a cambiar interactivamente e
iterativmanete.x

EL ejemplo que quiero dar de clasificacion es gradient descent que es 
basicamente n algoritmo que va 
